# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Pc86KPO708liMr81m4yECzN-ie5Cpfw
"""

# ============================================
# üìò STUDYMATE ‚Äî FIXED & FULL WORKING VERSION
# Google Colab | Gradio UI | Granite 3.3 2B Instruct
# ============================================

# ---------------------
# Install Dependencies
# ---------------------
!pip install -q gradio pymupdf sentence-transformers faiss-cpu transformers accelerate bitsandbytes


# ---------------------
# Imports
# ---------------------
import gradio as gr
import fitz  # PyMuPDF
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


# ==========================================================
# 1) LOAD GRANITE 3.3 2B INSTRUCT (HuggingFace)
# ==========================================================
model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)


# ==========================================================
# 2) LOAD EMBEDDING MODEL
# ==========================================================
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")


# ==========================================================
# 3) EXTRACT TEXT FROM PDFS
# ==========================================================
def extract_text_from_pdfs(pdf_files):
    full_text = ""
    for pdf in pdf_files:
        data = pdf.read()
        doc = fitz.open(stream=data, filetype="pdf")
        for page in doc:
            full_text += page.get_text()
    return full_text


# ==========================================================
# 4) CHUNKING FUNCTION
# ==========================================================
def chunk_text(text, chunk_size=350):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks


# ==========================================================
# 5) BUILD FAISS INDEX
# ==========================================================
def build_faiss(chunks):
    embeddings = embedding_model.encode(chunks)
    embeddings = np.array(embeddings).astype("float32")

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, embeddings


# ==========================================================
# 6) SEMANTIC SEARCH
# ==========================================================
def semantic_search(query, index, chunks, k=4):
    q_emb = embedding_model.encode([query]).astype("float32")
    D, I = index.search(q_emb, k)
    return [chunks[i] for i in I[0]]


# ==========================================================
# 7) LLM ANSWER GENERATION
# ==========================================================
def generate_answer(question, retrieved_chunks):
    context = "\n".join(retrieved_chunks)

    prompt = f"""
You are StudyMate, an AI academic assistant.
Answer ONLY using the context provided.

Context:
{context}

Question: {question}

Answer clearly with references to the retrieved text.
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    output = model.generate(
        **inputs,
        max_new_tokens=250,
        temperature=0.2,
        do_sample=False
    )

    reply = tokenizer.decode(output[0], skip_special_tokens=True)

    # Remove prompt repetition
    if "Answer clearly" in reply:
        reply = reply.split("Answer clearly")[-1].strip()

    return reply


# ==========================================================
# 8) GLOBAL MEMORY
# ==========================================================
global_chunks = None
global_index = None


# ==========================================================
# 9) PROCESS PDF BUTTON
# ==========================================================
def process_pdfs(pdf_files):
    global global_chunks, global_index

    if not pdf_files:
        return "‚ùå No PDF uploaded. Please upload a file."

    text = extract_text_from_pdfs(pdf_files)
    chunks = chunk_text(text)
    index, _ = build_faiss(chunks)

    global_chunks = chunks
    global_index = index

    return "‚úÖ PDF processed successfully! You can now ask questions."


# ==========================================================
# 10) ANSWER USER QUESTION
# ==========================================================
def ask_question(question, history):
    global global_chunks, global_index

    if global_chunks is None:
        return history + [["System", "‚ö†Ô∏è Please upload and process PDFs first."]]

    retrieved = semantic_search(question, global_index, global_chunks)

    answer = generate_answer(question, retrieved)

    history = history + [[question, answer]]

    return history


# ==========================================================
# 11) GRADIO APP UI
# ==========================================================
with gr.Blocks() as app:

    gr.Markdown("# üìò StudyMate ‚Äî AI Academic Assistant\nUpload PDFs and ask anything!")

    pdf_upload = gr.Files(label="Upload PDF files", file_types=[".pdf"])
    process_btn = gr.Button("Process PDFs")
    status_box = gr.Textbox(label="Status")

    chatbot = gr.Chatbot(label="StudyMate Chat")
    question_box = gr.Textbox(label="Ask a question")
    ask_btn = gr.Button("Send")

    process_btn.click(process_pdfs, inputs=pdf_upload, outputs=status_box)

    ask_btn.click(
        ask_question,
        inputs=[question_box, chatbot],
        outputs=chatbot
    )

app.launch()