# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S2rpIlGOkuxCrDCIhehGviiR6yMXrEZM
"""

# --------------------------------------------------------
# INSTALL DEPENDENCIES
# --------------------------------------------------------
!pip install transformers accelerate pypdf gradio sentencepiece --quiet

# --------------------------------------------------------
# IMPORTS
# --------------------------------------------------------
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from pypdf import PdfReader
import torch

# --------------------------------------------------------
# LOAD GRANITE 3B (CAUSAL LM)
# --------------------------------------------------------
model_name = "ibm-granite/granite-3b-code-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",        # Uses GPU if available
    torch_dtype=torch.float16 # Faster on T4 GPU
)

# --------------------------------------------------------
# PDF TEXT EXTRACTOR
# --------------------------------------------------------
def extract_pdf(pdf_file):
    try:
        reader = PdfReader(pdf_file.name)
        full_text = ""
        for page in reader.pages:
            page_text = page.extract_text() or ""
            full_text += page_text

        if len(full_text.strip()) == 0:
            return None, "PDF has no extractable text (might be scanned)."

        # limit to avoid GPU overload
        return full_text[:8000], None

    except Exception as e:
        return None, f"PDF Error: {str(e)}"

# --------------------------------------------------------
# QA FUNCTION
# --------------------------------------------------------
def ask(pdf, question):
    try:
        if pdf is None:
            return "‚ùå Upload a PDF first."

        text, err = extract_pdf(pdf)
        if err:
            return err

        prompt = (
            f"You are an academic assistant.\n"
            f"Here is the study material:\n{text}\n\n"
            f"Question: {question}\n"
            f"Answer:"
        )

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        output_tokens = model.generate(
            **inputs,
            max_new_tokens=250,
            temperature=0.2,
            do_sample=False
        )

        answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

        # Remove prompt part
        if "Answer:" in answer:
            answer = answer.split("Answer:")[-1].strip()

        return answer

    except Exception as e:
        return f"Runtime Error: {str(e)}"

# --------------------------------------------------------
# GRADIO UI
# --------------------------------------------------------
with gr.Blocks() as ui:
    gr.Markdown("## üìò StudyMaze AI ‚Äî PDF Q&A (Granite 3B)")

    pdf = gr.File(label="Upload Study PDF")
    q = gr.Textbox(label="Ask a question from the PDF")
    btn = gr.Button("Submit")
    ans = gr.Textbox(label="Answer")

    btn.click(fn=ask, inputs=[pdf, q], outputs=ans)

ui.launch()